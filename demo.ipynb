{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case1 (Offline refinement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我认为EMNLP是世界上最好的NLP会议！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from utils.utils import LANG_TABLE\n",
    "from utils.build_dataset import get_inter_prompt, get_plain_prompt\n",
    "from inference import get_pair_suffix, clean_outputstring\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "# Load base model and LoRA weights\n",
    "base_model_path = \"google/gemma-7b\"\n",
    "peft_path = \"fzp0424/Ladder-7B-LoRA\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype='auto', device_map = device)\n",
    "model = PeftModel.from_pretrained(model, peft_path, torch_dtype='auto', device_map = device)\n",
    "model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, padding_side='left')\n",
    "\n",
    "\n",
    "test_case = {\n",
    "        \"translation\": {\n",
    "            \"pair\": \"en-zh\",\n",
    "            \"en\": \"I think EMNLP is the best NLP conference in the world!\",\n",
    "            \"medium\": \"我认为EMNLP是最棒的会议\",\n",
    "            \"shots\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "item = test_case[\"translation\"]\n",
    "shots = item['shots'] \n",
    "src_lan = item['pair'].split(\"-\")[0]\n",
    "tgt_lan = item['pair'].split(\"-\")[1]\n",
    "prompt = get_inter_prompt(src_lan, tgt_lan, item, shots)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).input_ids.to(device)\n",
    "\n",
    "# Translation\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=256, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "suffix = get_pair_suffix(tgt_lan) \n",
    "suffix_count = output[0].count(suffix)\n",
    "pred = clean_outputstring(output[0], suffix, logger, suffix_count)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case2 (Online refinement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "from utils.utils import LANG_TABLE\n",
    "from utils.build_dataset import get_inter_prompt, get_plain_prompt\n",
    "from inference import get_pair_suffix, get_plain_suffix, clean_outputstring\n",
    "\n",
    "# Initialize logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "# Paths to model directories\n",
    "BASE_MODEL_PATH = \"google/gemma-2b\"\n",
    "PEFT_MODEL_PATH = \"fzp0424/Ladder-2B-LoRA\"\n",
    "TARGET_MODEL_PATH = \"google/gemma-2b-it\" #use gemma-2b-it as the target model\n",
    "\n",
    "# Load base model and LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_PATH, torch_dtype='auto', device_map=device)\n",
    "peft_model = PeftModel.from_pretrained(base_model, PEFT_MODEL_PATH, torch_dtype='auto', device_map=device)\n",
    "peft_model.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, padding_side='left')\n",
    "\n",
    "# Load target tokenizer and model\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(TARGET_MODEL_PATH, device_map=device)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(TARGET_MODEL_PATH, device_map=device)\n",
    "\n",
    "# Test case for translation\n",
    "test_case = {\n",
    "    \"translation\": {\n",
    "        \"pair\": \"en-zh\",\n",
    "        \"en\": \"The document, based on the Anti-Secession Law, the Criminal Law and the Criminal Procedure Law, provides more specific rules concerning conviction and sentencing in the event of such crimes, as well as relevant procedures, serving as guidance for the judiciary in handling relevant cases.\",\n",
    "        \"shots\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Extract source and target languages\n",
    "src_lang = test_case[\"translation\"][\"pair\"].split(\"-\")[0]\n",
    "tgt_lang = test_case[\"translation\"][\"pair\"].split(\"-\")[1]\n",
    "\n",
    "# Generate medium translation using target model\n",
    "plain_prompt = get_plain_prompt(src_lang, tgt_lang, test_case[\"translation\"])\n",
    "medium_input_ids = target_tokenizer(plain_prompt, return_tensors=\"pt\").to(device)\n",
    "medium_outputs = target_model.generate(**medium_input_ids, num_beams=5, max_new_tokens=256, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "medium_output_text = target_tokenizer.decode(medium_outputs[0], skip_special_tokens=True)\n",
    "plain_suffix = get_plain_suffix(tgt_lang)\n",
    "plain_suffix_count = medium_output_text.count(plain_suffix)\n",
    "medium_translation = clean_outputstring(medium_output_text, plain_suffix, logger, plain_suffix_count)\n",
    "print(\"Raw Translation:\\n\", medium_translation)\n",
    "\n",
    "# Update test case with medium translation\n",
    "test_case[\"translation\"][\"medium\"] = medium_translation\n",
    "\n",
    "# Generate refined translation using Ladder\n",
    "inter_prompt = get_inter_prompt(src_lang, tgt_lang, test_case[\"translation\"])\n",
    "input_ids = tokenizer(inter_prompt, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).input_ids.to(device)\n",
    "\n",
    "# Translation with Ladder\n",
    "with torch.no_grad():\n",
    "    refined_outputs = peft_model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=256, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "refined_output_text = tokenizer.batch_decode(refined_outputs, skip_special_tokens=True)[0]\n",
    "pair_suffix = get_pair_suffix(tgt_lang)\n",
    "pair_suffix_count = refined_output_text.count(pair_suffix)\n",
    "refined_translation = clean_outputstring(refined_output_text, pair_suffix, logger, pair_suffix_count)\n",
    "\n",
    "print(\"Ladder-Refined:\\n\", refined_translation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('chatglm_etuning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8a1dd910db45e0c3361d702c58dd23e0f2853fe049b9fce34ed5e096bc7f57e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
